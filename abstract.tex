Although our understanding of traffic patterns in computer networks is vastly deeper than it was twenty years ago, we still are missing a ``cookbook'' for generating realistic workload in local area networks ({\footnotesize{LAN}}s). Worse still, we are without a commonly agreed-upon definition of what it means for a traffic model to be realistic. This inability to produce realistic wireless {\footnotesize{LAN}} workload is no mere academic concern: we believe it is one of the major reasons that wireless experiments---simulated or otherwise---so rarely succeed in predicting real-world performance of new protocols. Our results indicate that na\"ive but commonly used traffic models can induce drastic distortion of vital performance metrics, such as end-to-end application-level network delay, and jitter, as well as important metrics at other layers of the network. These errors are great than an order of magnitude more than a quarter of the diverse simulation scenarios we have evaluated. In this work we present an objective, testable definition of ``sufficient realism'' for traffic models, and develop the theoretical methodology necessary to interpret experimental results using this definition. We also propose the Traffic Object Model~({\footnotesize{TOM}}) as a standard framework for implementing reusable, modular {\footnotesize{LAN}} traffic models.
%
%When new wireless technologies are deployed and subjected to real usage patterns, unforeseen performance problems inevitably seem to arise, to be fixed only in later generations. Why do these performance issues fail to appear in experimental settings before the technology is deployed? We believe that one of the major reasons behind the discrepancies found between experimental performance evaluations and real-world experience lies in the unrealistic workload patterns typically used in experiments. One of the significant contributions of this work is to rigorously demonstrate that common synthetic traffic models for wireless local-area networks induce drastically distorted performance metrics at every layer of the protocol stack. In order to show this, we present a testable definition of ``sufficient realism'' for traffic models, and develop the theoretical methodology necessary to interpret experimental results using this definition. Finally, we show by example that this distortion can completely invert the relative performance of protocols. The greater overall contribution of this paper, however, is the complete collection of ideas, techniques and analytical tools that will allow the development of more realistic synthetic traffic models in the future.
%
%
%Performance predictions from simulations in wireless networking rarely seem to match the behavior observed once the same technologies are deployed. We believe that one of the major factors hampering researchers' ability to make more reliable forecasts is the inability to generate realistic experimental workloads. To redress this problem, we take a fundamentally new approach to quantifying the realism of wireless traffic models. In this approach, the realism of a model is defined directly in terms of its ability to accurately reproduce the performance characteristics of actual network usage. This direct approach cuts through the Gordian knot of deciding which statistical features of traffic traces are significant. The first major contribution of this work is this new definition of workload realism, together with the analytical and statistical methodology to rigorously assess whether synthetic traffic models meet the definition. The second contribution is the conclusion that commonly used models of wireless traffic distort important metrics for performance evaluation at every layer of the wireless protocol stack. We show by example that this distortion can completely invert the relative performance of protocols. The last and most important contribution is the complete collection of ideas, techniques and analytical tools that will allow the development of more realistic synthetic traffic models in the future.
